# Analytics Server Docker

## Introduction

To demonstrate the full end to end capabilities and to help developers jump start their development, Deepstream 3.0 comes with a complete reference 
implementation of a smart parking solution. This reference application can be deployed on edge servers or in the cloud. 
Developers can leverage this and adapt to their specific use cases. Docker containers have been provided to further simplify deployment, adaptability, and manageability.

The architecture of the application looks as follows:

![Architecture](readme-images/architecture.png?raw=true "Architecture")

**Note**: This application creates docker containers only for Analytics Server.

The application can be run in two modes:
+ **Playback**: This mode is used to playback events from a point in time
+ **Live**: This mode is used for seeing the events, scene as and when they are detected

## Getting Started

### Dependencies

The application requires recent versions of [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/) and [Docker Compose](https://docs.docker.com/compose/install/#install-compose) to be installed in the machine.

### Environment Variables

Export the following Environment variables:
+ **IP_ADDRESS** - IP address of host machine
+ **GOOGLE_MAP_API_KEY** - Api Key for Google Map

Follow the instructions in this [link](https://developers.google.com/maps/documentation/javascript/get-api-key) to get an api key for Google Maps.

### Configurations

Playback is the default mode of the application.

If live mode has to be used, then:
    
1. Go to `node-apis/config/config.json` and change the following config:

        garage.isLive: true

2. Send the data generated by DeepStream 3.0 to the kafka topic `metromind-raw`

### Installation

1. Install Docker and Docker Compose.

2. Change Configurations (Optional)

3. Starting Docker Containers

    The dockers can be started either by running automation script or by starting them manually.

    **Automation Script**

    1. Open `start.sh` and replace `xxx.xxx.xx.xx` by the IP ADDRESS of the host machine and `<YOUR GOOGLE_API_KEY>` with your own GOOGLE API KEY.
    
    2. Run `start.sh` by executing the following command
    
            ./start.sh
    
    3. Execute the following command to stop the containers
    
            sudo docker-compose down
    
    **Note**:
    + `sudo docker-compose down` should be used to stop the containers. The execution of this command significantly reduces the time taken by docker containers to start again when `start.sh` is executed.
    + `stop.sh` should be only used when the containers need to be stopped and the docker images have to be removed from the system. `stop.sh` can be run using the following command
    
            ./stop.sh
    
    **Steps to start the containers manually**
        
    1. Export the environment variables

        a) IP Address of Host machine
        
            export IP_ADDRESS=xxx.xxx.xx.xx

        b) Google Map API Key:
    
            export GOOGLE_MAP_API_KEY=<YOUR GOOGLE_API_KEY>
    
    2. Assuming that the application has been cloned from this repository

            git clone https://github.com/NVIDIA-AI-IOT/deepstream_360_d_smart_parking_application.git
        
       use the following command to change the current directory.

            cd ./analytics_server_docker
         
    3. Run the docker containers using the following `docker-compose` command
    
            sudo -E docker-compose up -d

       this will start the following containers

            cassandra
            kafka
            zookeeper
            spark-master
            spark-worker
            elasticsearch
            kibana
            logstash
            api
            ui
            python-module

    4.  Start spark streaming job, this job does the following
    
        a) manages the state of parking garage

        b) detects car "understay" anomaly

        c) computes flowrate

        run the following command to login into spark master 
    
            sudo docker exec -it spark-master /bin/bash

        the docker container picks up the jar file from spark/data

            ./bin/spark-submit  --class com.nvidia.ds.stream.StreamProcessor --master spark://master:7077 --executor-memory 8G --total-executor-cores 4 /tmp/data/stream-360-1.0-jar-with-dependencies.jar
    
    
        Note that one can go to stream directory and compile the source code using maven to create the stream-360-1.0-jar-with-dependencies.jar

            mvn clean install -Pjar-with-dependencies

    
    
    **Note**:
    + The deepstream application should be started only after the analytics server is up and running.
    + Remember to shut down the docker-containers of analytics server once the deepstream is shut down.

4. Start spark batch job, this detects "overstay" anomaly.

    Use a second shell and run the following command to login into spark master 

        sudo docker exec -it spark-master /bin/bash
    
    run the batch job
    
        ./bin/spark-submit  --class com.nvidia.ds.batch.BatchAnomaly --master local[8]  /tmp/data/stream-360-1.0-jar-with-dependencies.jar

5.  **Generate Data** (Optional) , for test purpose ONLY, normally Deepstream Smart Parking application will read from camera and send metadata to Analytics Server 

        a) sudo apt-get update
        b) sudo apt install openjdk-8-jdk
        c) sudo apt-get install maven 
        d) cd ../stream
        e) sudo mvn clean install exec:java -Dexec.mainClass=com.nvidia.ds.util.Playback -Dexec.args="<KAFKA_BROKER_IP_ADDRESS>:<PORT> --input-file <path to input file>"

    **Note**: 
    + Change KAFKA_BROKER_IP_ADDRESS and PORT with Host IP_ADDRESS and port used by Kafka respectively.
    + Set path to input file as `data/playbackData.json` for viewing the demo data.
    + The following additional options can be added to args in `step e`:
        + **topic-name** - Name of the kafka topic to which data has to be sent. Set it to `metromind-raw` if input data is not tracked, but if input data has already gone through the tracking module then send it to `metromind-start`. The default value used in step e is `metromind-start`.<br/>
    With this additional option, `step e` will look as follows:
        
                sudo mvn clean install exec:java -Dexec.mainClass=com.nvidia.ds.util.Playback -Dexec.args="<KAFKA_BROKER_IP_ADDRESS>:<PORT> --input-file <path to input file> --topic-name <kafka topic name>"
                
6. **Create Elasticsearch start-Index** (Optional)

    browse to Kibana URL http://IP_ADDRESS:5601

     ![Start Index](readme-images/index-creation-1.png?raw=true "Start Index")


7. **Create Elasticsearch anomaly-Index** (Optional)

    ![Anomaly Index](readme-images/index-creation-2.png?raw=true "Anomaly Index")


8. **Test**
    
    http://IP_ADDRESS
    
    ![UI](readme-images/ui.png?raw=true "UI")    

    **Note**: The events that show up in the UI are comparatively less as compared to real events. This is because, if a object has a lot of events within the refresh interval then the events with respect to other objects may get obscured. To avoid this situation we display only a few events per object.
    
### References

This analytics server has multiple components like Kafka, Elasticsearch, Nodejs etc. Use the following links to get started with these components.

1. [Cassandra](http://cassandra.apache.org/doc/latest/getting_started/)
2. [Kafka](https://kafka.apache.org/intro)
3. [Zookeeper](https://zookeeper.apache.org/)
4. [Spark](https://spark.apache.org/docs/latest/quick-start.html)
5. [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/6.4/getting-started.html)
6. [Kibana](https://www.elastic.co/guide/en/kibana/6.4/getting-started.html)
7. [Logstash](https://www.elastic.co/guide/en/logstash/6.4/getting-started-with-logstash.html)
8. [Node.js](https://nodejs.org/en/docs/guides/getting-started-guide/)
9. [React](https://reactjs.org/docs/getting-started.html)
10. [Python](https://docs.python.org/3/tutorial/)
11. [Scala](https://docs.scala-lang.org/getting-started.html)
